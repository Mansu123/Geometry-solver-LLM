# Codeforces Geometry Problem Solver with Ollama

An automated system that uses local LLMs (via Ollama) to solve geometry problems from the Codeforces dataset. Optimized for Mac M1 GPU.

## üéØ Features

- ‚úÖ Filters geometry problems from open-r1/codeforces dataset
- ‚úÖ Generates solutions using any Ollama model
- ‚úÖ Executes code against test cases
- ‚úÖ Comprehensive logging and statistics
- ‚úÖ Mac M1 GPU optimized
- ‚úÖ Works with any Ollama model

## üìã Prerequisites

- **Python 3.8+**
- **Ollama** installed and running
- **Mac M1** (or any system with Ollama support)

## üöÄ Installation

### 1. Install Ollama

```bash
# Install Ollama on Mac
brew install ollama

# Start Ollama service
ollama serve
```

### 2. Pull a Model

```bash
# Recommended for coding tasks
ollama pull codellama

# Or use other models
ollama pull llama3.2
ollama pull deepseek-coder
ollama pull phi3
```

### 3. Setup Python Environment

```bash
# Clone or navigate to project directory
cd codeforces_geometry_solver

# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Mac/Linux

# Install dependencies
pip install --break-system-packages -r requirements.txt

# Or if you're using venv:
pip install -r requirements.txt
```

## üì¶ Project Structure

```
codeforces_geometry_solver/
‚îÇ
‚îú‚îÄ‚îÄ main.py                 # Main entry point
‚îú‚îÄ‚îÄ config.py              # Configuration settings
‚îú‚îÄ‚îÄ dataset_loader.py      # Dataset loading and filtering
‚îú‚îÄ‚îÄ model_interface.py     # Ollama API integration
‚îú‚îÄ‚îÄ code_executor.py       # Safe code execution
‚îú‚îÄ‚îÄ evaluator.py          # Testing and evaluation
‚îú‚îÄ‚îÄ requirements.txt       # Python dependencies
‚îî‚îÄ‚îÄ README.md             # This file
```

## üéÆ Usage

### Basic Usage

```bash
# Run with default settings (llama3.2, all geometry problems)
python main.py

# Use specific model
python main.py --model codellama

# Limit number of problems
python main.py --model codellama --max-problems 10

# Use custom Ollama server
python main.py --ollama-url http://192.168.1.100:11434
```

### Command Line Options

```bash
python main.py --help

Options:
  --model MODEL           Ollama model to use (default: llama3.2)
  --max-problems N        Maximum number of problems to evaluate
  --ollama-url URL        Ollama server URL (default: http://localhost:11434)
  --timeout SECONDS       Execution timeout per test (default: 10)
  --list-models          List available Ollama models
```

### Examples

```bash
# Evaluate first 5 problems with CodeLlama
python main.py --model codellama --max-problems 5

# Use DeepSeek Coder for better code generation
python main.py --model deepseek-coder

# Quick test with 3 problems
python main.py --max-problems 3

# List available models
python main.py --list-models
```

## üìä Output

The script generates several output files:

1. **results_YYYYMMDD_HHMMSS.json** - Detailed results including generated code
2. **geometry_results.csv** - Summary CSV for easy analysis
3. **Console output** - Real-time progress and statistics

### Sample Output

```
[1/15] Problem: 1/A
Title: Theatre Square
Tags: ['geometry', 'math']
  Generating solution with codellama... ‚úì
  Testing with 3 test cases...
    Test 1: ‚úì PASS (0.023s)
    Test 2: ‚úì PASS (0.019s)
    Test 3: ‚úì PASS (0.021s)
  Result: 3/3 tests passed (100.0%)
------------------------------------------------------------

EVALUATION SUMMARY
====================================================================
Total problems evaluated: 15
Code generated successfully: 15/15

Total test cases: 45
Tests passed: 38/45
Overall accuracy: 84.44%

Problems solved perfectly: 12/15
Average accuracy per problem: 84.44%
```

## ‚öôÔ∏è Configuration

Edit `config.py` to customize:

```python
# Model settings
DEFAULT_MODEL = "codellama"  # Change default model
OLLAMA_BASE_URL = "http://localhost:11434"

# Execution settings
EXECUTION_TIMEOUT = 10  # Seconds per test case
MAX_OUTPUT_LENGTH = 10000

# Dataset settings
TARGET_TAG = "geometry"  # Change to filter other tags
```

## üîß Troubleshooting

### Ollama not connecting

```bash
# Check if Ollama is running
ps aux | grep ollama

# Start Ollama
ollama serve

# Test connection
curl http://localhost:11434/api/tags
```

### Model not found

```bash
# List available models
ollama list

# Pull missing model
ollama pull codellama
```

### Python package issues

```bash
# Mac M1 specific fix
pip install --break-system-packages datasets ollama pandas numpy tqdm

# Or use virtual environment
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### Dataset loading issues

```bash
# Login to Hugging Face (if needed)
huggingface-cli login

# Or set token in code
# from huggingface_hub import login
# login(token="your_token_here")
```

## üß™ Testing Individual Components

### Test Dataset Loader

```bash
python dataset_loader.py
```

### Test Ollama Interface

```bash
python model_interface.py
```

### Test Code Executor

```bash
python code_executor.py
```

### Test Evaluator

```bash
python evaluator.py
```

## üéØ Recommended Models

For coding tasks, these models work best:

1. **codellama** - Meta's specialized coding model (recommended)
2. **deepseek-coder** - Excellent for complex problems
3. **llama3.2** - Good general purpose model
4. **phi3** - Fast and efficient for simpler problems

```bash
# Pull recommended models
ollama pull codellama
ollama pull deepseek-coder
```

## üìà Performance Tips

### Mac M1 Optimization

Ollama automatically uses the Metal framework for GPU acceleration on M1. No additional configuration needed!

### Speed up evaluation

```bash
# Use smaller, faster models for initial testing
python main.py --model phi3 --max-problems 5

# Use larger models for better accuracy
python main.py --model codellama --max-problems 50
```

### Reduce timeout for faster iteration

```bash
python main.py --timeout 5 --max-problems 10
```

## üêõ Debugging

Enable verbose output by modifying code:

```python
# In model_interface.py
def generate_solution_streaming(self, problem_details: dict):
    # Use this for real-time model output
    pass
```

## üìù License

This project uses the open-r1/codeforces dataset. Please respect Codeforces' terms of use.

## ü§ù Contributing

Feel free to submit issues or pull requests!

## üìö Additional Resources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Codeforces](https://codeforces.com/)
- [open-r1 Dataset](https://huggingface.co/datasets/open-r1/codeforces)

## ‚ö° Quick Start Checklist

- [ ] Install Ollama: `brew install ollama`
- [ ] Start Ollama: `ollama serve`
- [ ] Pull a model: `ollama pull codellama`
- [ ] Install Python packages: `pip install -r requirements.txt`
- [ ] Run evaluation: `python main.py --max-problems 5`
- [ ] Check results: `cat geometry_results.csv`

## üí° Tips

- Start with `--max-problems 5` to test your setup
- Use `codellama` or `deepseek-coder` for best coding results
- Check the JSON output for detailed error analysis
- The script automatically uses Mac M1 GPU through Ollama's Metal backend
